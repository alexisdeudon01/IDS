# ============================================================================
# Vector Configuration for IDS2 SOC Pipeline
# Raspberry Pi 5 Optimized
# 
# This file is AUTO-GENERATED by vector_manager.py
# Manual edits will be overwritten on agent restart
# ============================================================================

# Data directory for Vector's internal state
data_dir = "/var/lib/vector"

# ============================================================================
# SOURCES - Where Vector reads data from
# ============================================================================

# Source: Read Suricata EVE JSON logs from RAM disk
[sources.suricata_logs]
type = "file"
include = ["/mnt/ram_logs/eve.json"]
read_from = "end"  # Start reading from end (don't reprocess old logs)
fingerprint.strategy = "device_and_inode"  # Track file identity
max_line_bytes = 102400  # 100KB max line size (handles large events)

# Additional HTTP ingestion source for E2E testing
[sources.http_ingest]
type = "http"
address = "0.0.0.0:8282"
decoding.codec = "json"

# ============================================================================
# TRANSFORMS - Data processing and enrichment
# ============================================================================

# Transform 1: Parse JSON from Suricata
[transforms.parse_json]
type = "remap"
inputs = ["suricata_logs"]
source = """
# Parse the JSON line
. = parse_json!(.message)
"""

# Passthrough for HTTP source (no-op, ensures schema compatibility)
[transforms.passthrough_http]
type = "remap"
inputs = ["http_ingest"]
source = """
. = .
"""

# Transform 2: Add Elastic Common Schema (ECS) fields
[transforms.add_ecs_fields]
type = "remap"
inputs = ["parse_json", "passthrough_http"]
source = """
# ============================================================================
# ECS Base Fields
# ============================================================================
.@timestamp = .timestamp
.ecs.version = "8.0.0"

# ============================================================================
# Event Fields
# ============================================================================
.event.kind = "event"
.event.category = ["network"]
.event.type = ["connection"]
.event.dataset = "suricata.eve"
.event.module = "suricata"

# ============================================================================
# Source/Destination IP and Ports
# ============================================================================
if exists(.src_ip) {
    .source.ip = .src_ip
    .source.port = .src_port
}

if exists(.dest_ip) {
    .destination.ip = .dest_ip
    .destination.port = .dest_port
}

# ============================================================================
# Network Protocol
# ============================================================================
if exists(.proto) {
    .network.protocol = downcase(.proto)
}

# ============================================================================
# Alert Fields (if this is an alert event)
# ============================================================================
if exists(.alert) {
    .event.kind = "alert"
    .event.category = ["intrusion_detection"]
    .event.type = ["denied"]
    .rule.name = .alert.signature
    .rule.id = to_string(.alert.signature_id)
    .event.severity = .alert.severity
    
    # Alert metadata
    if exists(.alert.category) {
        .rule.category = .alert.category
    }
    
    if exists(.alert.gid) {
        .rule.ruleset = to_string(.alert.gid)
    }
}

# ============================================================================
# Flow Fields (if this is a flow event)
# ============================================================================
if exists(.flow) {
    .network.bytes = .flow.bytes_toserver + .flow.bytes_toclient
    .network.packets = .flow.pkts_toserver + .flow.pkts_toclient
    
    .source.bytes = .flow.bytes_toserver
    .source.packets = .flow.pkts_toserver
    
    .destination.bytes = .flow.bytes_toclient
    .destination.packets = .flow.pkts_toclient
}

# ============================================================================
# HTTP Fields (if this is an HTTP event)
# ============================================================================
if exists(.http) {
    .http.request.method = .http.http_method
    .http.request.referrer = .http.http_refer
    .url.original = .http.url
    .url.domain = .http.hostname
    .http.response.status_code = .http.status
    .http.version = .http.protocol
}

# ============================================================================
# DNS Fields (if this is a DNS event)
# ============================================================================
if exists(.dns) {
    .dns.question.name = .dns.rrname
    .dns.question.type = .dns.rrtype
    
    if exists(.dns.rcode) {
        .dns.response_code = .dns.rcode
    }
}

# ============================================================================
# TLS Fields (if this is a TLS event)
# ============================================================================
if exists(.tls) {
    .tls.server.subject = .tls.subject
    .tls.server.issuer = .tls.issuerdn
    .tls.version = .tls.version
    
    if exists(.tls.sni) {
        .tls.client.server_name = .tls.sni
    }
}

# ============================================================================
# Agent Metadata
# ============================================================================
.agent.type = "vector"
.agent.version = "0.34.0"
.host.hostname = "192.168.178.66"
.host.architecture = "aarch64"
.host.ip = "192.168.178.66"

# ============================================================================
# Cleanup - Remove original message field
# ============================================================================
del(.message)
"""

# Transform 3: Add index routing
[transforms.route_to_index]
type = "remap"
inputs = ["add_ecs_fields"]
source = """
# Generate index name based on date (YYYY.MM.DD format)
.index_name = "ids2-logs-" + format_timestamp!(.@timestamp, format: "%Y.%m.%d")
"""

# ============================================================================
# SINKS - Where Vector sends data to
# ============================================================================

# Sink 1: AWS OpenSearch (Primary destination)
[sinks.opensearch]
type = "elasticsearch"
inputs = ["route_to_index"]
endpoint = "https://search-ids2-soc-domain-7p7ddhpiegpwgtk77rn7xn53v4.us-east-1.es.amazonaws.com"
mode = "bulk"

# Index configuration
bulk.index = "{{ index_name }}"
bulk.action = "create"

# Compression (reduces network bandwidth)
compression = "gzip"

# Batch settings (optimized for Raspberry Pi)
batch.max_events = 100
batch.timeout_secs = 30

# Buffer settings (disk-based for reliability)
buffer.type = "disk"
buffer.max_size = 268435456  # 256MB
buffer.when_full = "block"

# Request settings
request.timeout_secs = 60
request.retry_attempts = 3
request.retry_initial_backoff_secs = 2
request.retry_max_duration_secs = 300

# Health check
healthcheck.enabled = true
healthcheck.uri = "https://search-ids2-soc-domain-7p7ddhpiegpwgtk77rn7xn53v4.us-east-1.es.amazonaws.com/_cluster/health"

# Encoding
encoding.codec = "json"

# TLS settings
tls.verify_certificate = true
tls.verify_hostname = true

# AWS authentication (uses IAM credentials from profile)
auth.strategy = "aws"
auth.assume_role = ""

# ============================================================================
# Sink 2: Redis (Fallback buffer when OpenSearch is slow/unavailable)
# ============================================================================

[sinks.redis_fallback]
type = "redis"
inputs = ["route_to_index"]
url = "redis://redis:6379/0"
key = "vector:fallback:{{ index_name }}"
data_type = "list"
list.method = "rpush"

# Batch settings (smaller batches for fallback)
batch.max_events = 50
batch.timeout_secs = 10

# Encoding
encoding.codec = "json"

# Only use when OpenSearch is slow
# This provides a secondary buffer layer

# ============================================================================
# INTERNAL METRICS - Expose Vector's internal metrics
# ============================================================================

# Source: Vector's internal metrics
[sources.internal_metrics]
type = "internal_metrics"

# Sink: Prometheus exporter
[sinks.prometheus_exporter]
type = "prometheus_exporter"
inputs = ["internal_metrics"]
address = "0.0.0.0:9101"
default_namespace = "vector"

# Expose metrics for:
# - Events processed
# - Buffer usage
# - Sink health
# - Processing latency

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================

[log_schema]
host_key = "host"
message_key = "message"
timestamp_key = "@timestamp"

# ============================================================================
# NOTES
# ============================================================================
# 
# This configuration:
# 1. Reads Suricata EVE JSON logs from /mnt/ram_logs/eve.json
# 2. Parses and transforms to Elastic Common Schema (ECS)
# 3. Routes to daily indices (ids2-logs-YYYY.MM.DD)
# 4. Sends to AWS OpenSearch with disk buffering
# 5. Falls back to Redis if OpenSearch is slow
# 6. Exposes Prometheus metrics on port 9101
# 
# Resource usage:
# - CPU: ~1 core max
# - RAM: ~1GB max
# - Disk buffer: 256MB max
# - Network: Compressed with gzip
# 
# ============================================================================
